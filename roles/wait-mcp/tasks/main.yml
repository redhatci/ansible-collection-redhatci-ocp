---

- name: Wait for updated machine configs to be applied on the nodes
  block:
    - name: "Pause for Machine Config to be created"
      pause:
        seconds: 60

    - name: Get Machine config pools status
      k8s_info:
        api_version: machineconfiguration.openshift.io/v1
        kind: MachineConfigPool
        kubeconfig: "{{ kubeconfig_path }}"
      register: reg_mcpool_status
      vars:
        status_query: "resources[*].status.conditions[?type=='Updated'].status"
        update_status: "{{ reg_mcpool_status | json_query(status_query) | flatten | unique }}"
      until:
        - update_status == ['True']
      retries: "{{ mcp_wait_retries }}"
      delay: "{{ mcp_wait_delay }}"
      delegate_to: localhost

  rescue:
    # If the k8s_info module fails, we will try using OC to get the mcp status, because
    # the output may be different
    # We are doing this with ignore_errors: true just to retrieve logs in order to
    # compare the output with k8s_info output, in order to open a bugzilla
    - name: MachineConfigPool check failed, using oc mcp get check
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      shell: >
        {{ oc_tool_path }} get mcp
      register: reg_mcpool_status
      vars:
        master_query: "master[\\s]+rendered-master-[a-z0-9]+[\\s]+True[\\s]+False"
        worker_query: "worker[\\s]+rendered-worker-[a-z0-9]+[\\s]+True[\\s]+False"
        masters_updated: "{{ reg_mcpool_status.stdout | regex_search(master_query) != None }}"
        workers_updated: "{{ reg_mcpool_status.stdout | regex_search(worker_query) != None }}"
      until:
        - masters_updated
        - workers_updated
      retries: "{{ mcp_wait_retries }}"
      delay: "{{ mcp_wait_delay }}"
      ignore_errors: true
      delegate_to: localhost

    - name: Wait for updated machine configs to be applied on the nodes failed - no workaround applied
      fail:
        msg: "No workaround is defined, so the job has to fail at this point"
      when: '"bz2053445" not in dci_workarounds|default([])'

    # Bugzilla report: https://bugzilla.redhat.com/show_bug.cgi?id=2053445
    # Sometimes after applying MCPs some nodes remain unschedulable.
    - name: Workaround for BZ-2053445 - force workers uncordon
      block:
        - name: Check for workers with Scheduling disabled
          environment:
            KUBECONFIG: "{{ kubeconfig_path }}"
          shell: >
            {{ oc_tool_path }} get nodes --no-headers=true |
            grep 'Ready,SchedulingDisabled' | grep worker | awk '{ print $1 }'
          register: reg_disabled_nodes
          delegate_to: localhost

        - name: Uncordon workers if needed
          block:
            - name: Workaround - uncordon all ready workers with disabled scheduling
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"
              shell: >
                {{ oc_tool_path }} adm uncordon {{ item }}
              loop: "{{ reg_disabled_nodes.stdout.split('\n') }}"
              delegate_to: localhost

            - name: Wait for workers to be ready
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"
              shell: >
                {{ oc_tool_path }} get nodes --no-headers=true | grep worker
              register: nodes
              until:
                - '"SchedulingDisabled" not in nodes.stdout'
              retries: 30
              delay: 10
              delegate_to: localhost
          when: reg_disabled_nodes.stdout | length > 0
      when:
        - dci_workarounds is defined
        - '"bz2053445" in dci_workarounds'

...
